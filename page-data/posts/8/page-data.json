{"componentChunkName":"component---src-page-template-post-tsx","path":"/posts/8/","result":{"pageContext":{"post":{"id":8,"title":"ReDet 논문 해석","dateString":"2021-04-02","description":"2021년 3월 13일에 나온 따끈따끈한 논문 해석을 해보겠습니다. 논문을 이번에 처음 읽어봤고 아는게 많이 없어서 이해 안된 부분이 많고 그런 부분은 메모를 해놨다. 뭔 말인지 진짜 모르겠다.","mainImageUrl":"","postThemes":[{"name":"post","id":1}],"html":"<h1 id=\"redet-논문-리뷰\" style=\"position:relative;\"><a href=\"#redet-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0\" aria-label=\"redet 논문 리뷰 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ReDet 논문 리뷰</h1>\n<p>2021년 3월 13일에 나온 따끈따끈한 논문 해석을 해보겠습니다.\r\n논문을 이번에 처음 읽어봤고 아는게 많이 없어서 이해 안된 부분이 많고 그런 부분은 메모를 해놨다.\r\n뭔 말인지 진짜 모르겠다.</p>\n<h2 id=\"redet\" style=\"position:relative;\"><a href=\"#redet\" aria-label=\"redet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Redet</h2>\n<p><a href=\"https://arxiv.org/abs/2103.07733\">링크</a></p>\n<p>제목: ReDet: A Rotation-equivariant Detector for Aerial Object Detection</p>\n<p>코드: <a href=\"https://github.com/csuhan/ReDet\">깃허브 링크</a></p>\n<p>연구 주제: Computer Vision and Pattern Recognition</p>\n<h3 id=\"abstraction\" style=\"position:relative;\"><a href=\"#abstraction\" aria-label=\"abstraction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>abstraction</h3>\n<p>자연 이미지에서와는 다르게 항공 이미지에서의 객체는 임의의 방향성을 갖는다.\r\n그러므로 항공 객체의 방향을 해석하기 위해서는 더 많은 정보가 필요하다.\r\n일반적인 CNN 은 방향 변화를 명시적으로 모델링하지 않으므로 많은 양의 회전 증강 데이터가 필요합니다. (이미지 회전 시켜서 데이터로 쓰는 작업을 말하는 것 같다.)\r\n그러므로 이 논문에서는 회전 등변성 및 회전 불변성을 명시 적으로 해석하는 Rotation-equivariant성 Detetor (ReDet)를 제안합니다.\r\n네트워크에 회전 등변 네트워크를 detector 에 통합해서 방향을 정확하게 예측하고 모델 크기를 줄입니다.\r\n회전 등변 기능을 기반으로 RoI의 방향에 따라 등변 기능에서 회전 불변 기능을 적응 적으로 추출하는 Rotation-invariant RoI Align (RiRoI Align)도 제시합니다.\r\n여러 도전적인 항공 이미지 데이터 세트 DOTA-v1.0, DOTA-v1.5 및 HRSC2016에 대한 광범위한 실험을 통해 우리의 방법이 항공 물체 감지 작업에서 최첨단 성능을 달성 할 수 있음을 보여줍니다.\r\n이전 최고의 결과와 비교하여 ReDet은 DOTA-v1.0, DOTA-v1.5 및 HRSC2016에서 각각 1.2, 3.5 및 2.6mAP를 확보하면서 매개 변수 수를 60 % 줄였습니다 (313Mb 대 121Mb)</p>\n<ul>\n<li>몰라요\n<ul>\n<li>회전 불변성</li>\n<li>RoI</li>\n<li>회전 등변성</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<p>제약이없는 항공 이미지에서 정확한 물체 감지를 달성하기 위해 대부분은 회전 불변 특징을 추출하는 데 전념합니다.\r\n실제로 Rotated RoI (RRoI) wrapping 은 rotation-invariant features 를 추출하는 데 가장 흔히 사용된다.\r\n하지만 일반적인 CNN feature 들과 wrapping 한 RRoI 는 rotation-invariant feature 를 만들어 낼 수 없다.\r\nrotation invariance 은 더 큰 수용 능력을 가진 network, 더 많은 training 샘플들로 만들어진다.\r\nCNN에 회전된 이미지를 주는 것은 원래 이미지의 rotation feature map 과 다르다. (??)\r\nregion features warped from regular CNN feature 는 종종 불안정하고 방햔 변화에 민감하다.</p>\n<blockquote>\n<p>CNN 에 RRoI wrapping 한 것은 한계가 있다.</p>\n</blockquote>\n<p>최근에 나온 몇몇 방법은 CNN 을 더 큰 그룹으로 확장시키고 그룹 convolution 으로 rotation equivariance 를 얻는다.\r\n이 방법의 feature map 은 부가적인 방향 channel 을 갖는다. (?? channel??)\r\n하지만 보통의 rotation-equivariant features 에 쌓인 RRoI 를 직접 적용하는 것은 rotation-invariant features 를 만들지 못한다.\r\n왜냐하면 orientation channels 는 여전히 어긋나도 2D 평면의 region features 만 감싸기 때문이다.\r\nrotation-invariant features 을 정확하게 얻기 위해서 RRoI 의 방향에 따라서 orientation dimension of feature maps 을 조절해야한다.</p>\n<blockquote>\n<p>최근 나온 방법보다 더 좋은게 있는데 RRoI 방향에 따라 orientation dimension of feature maps 을 조절하는 것</p>\n</blockquote>\n<p>이 논문에서 rotation-equivariant features 을 통해 rotation-invariant features 을 정확하게 추출하기 위한 ReDet 을 제시한다.\r\n이 논문은 rotation-equivariant feature 추출과 rotation-invariant feature 추출 두 부분으로 이루어져있다.\r\n우선 rotation-equivariant features 추출을 위해서 rotation-equivariant networks 를 backbone 과 통합한다.\r\n하지만 직접적으로 RRoI warping 을 적용하는 것은 여전히 rotation-equivariant features 로 부터 rotation-invariant features 을 구할 수 없다.\r\n그러므로 novel Rotation-invariant RoI Align (RiRoI Align) 를 제시한다.\r\n이것은 spartial dimension 안 RRoI 경계 박스에 따른 region feature 를 감싼다.\r\n또 orientation channels 과 feature interpolation 을 바꾸면서 orientation dimesion 안 align features 도 감싼다. (??)</p>\n<blockquote>\n<p>더 정확학게 추출하기 위해서 우리는 ReDet 을 제시한다.</p>\n</blockquote>\n<p>항공 이미지 data set DOTA 로 광범위한 실험을 수행했다.\r\nHRSC2016 우리 방법의 효율성을 증명했다.\r\n우리는 고품질 항공 객체 탐색을 위한 rotation equivariance 와 rotation invariance 를 해석하는 Redet 을 제시한다.\r\nrotation equivariance 가 방향을 갖는 객체 탐색에 도입된 것은 이번이 처음이다.\r\n우리는 또한 rotation-equivariant features 로부터 rotation-invariant features 을 추출하기 위해서 novel RiRoI Align 를 설계했다.\r\n다른 RRoI warping 방법과는 다르게 RiRoI Align 은 spatial and orientation dimensions 에서 완전하게 rotation-invariant features\r\n우리의 방법은 DOTA-v1.0, DOTA-v1.5 및 HRSC2016에서 각각 최첨단 80.10, 76.80 및 90.46 mAP를 달성합니다. (??)\r\n이전의 최상의 결과와 비교하여 우리의 방법은 1.2, 3.5 및 2.6 mAP 향상을 얻었습니다. (??)\r\n기준과 비교하여 우리의 방법은 일관되고 실질적인 개선을 보여 주며 매개 변수 수를 60 % 감소시킵니다 (313Mb 대 121Mb). (??)\r\n또한, 우리의 방법은 더 나은 모델 크기 대 정확도 절충을 달성합니다. (??)</p>\n<blockquote>\n<p>Redet 제시와 novel RiRoI Align 설계를 했고 우리의 방법은 효과적이었다.</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>회전 불변 feature (rotation invariant feature)</li>\n<li>RRoI wrapping</li>\n<li>convolution</li>\n<li>Feature map</li>\n<li>region feature</li>\n<li>networks</li>\n<li>backbone</li>\n<li>spartial dimension</li>\n<li>orientation channels</li>\n<li>feature interpolation</li>\n<li>align features</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"related-works\" style=\"position:relative;\"><a href=\"#related-works\" aria-label=\"related works permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Works</h3>\n<h4 id=\"1-oriented-object-detection\" style=\"position:relative;\"><a href=\"#1-oriented-object-detection\" aria-label=\"1 oriented object detection permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(1) Oriented Object Detection</h4>\n<p>HBB 를 사용하는 대부분의 일반적인 객체 탐색과는 다르게 방향성을 갖는 객체 탐색은 OBB 를 사용한다.\r\nOBB 는 큰 aspect ratio 을 가진 항공물체를 탐색하는데 필수적이다.\r\n일반적인 객체 탐색 개발과 함께 방향성을 가진 객체 탐색에도 잘 설계된 방법이 제시돼왔다\r\n임의의 방향성을 갖는 객체 탐색을 하기 위해서 몇몇 방법은 많은 rotated anchors 를 채택했다.\r\n많은 rotated anchors 는 계산을 복잡하게 하지만 다양한 각도, 크기, aspect ratio 를 얻을 수 있다.\r\nDing 은 많은 anchor 를 피하기 위해 Horizontal RoIs (HRoIs) 를 RRoIs 로 변형시키기 위한 RoI Transformer 를 제시했다.\r\nGliding vertex 와 CenterMap 은 각각 방향이있는 객체를 정확하게 설명하기 위해 사각형과 마스크를 사용합니다.\r\nR3Det and S2A-Net 는 horizontal receptive fields 와 rotated anchor 사이에 feature 를 정렬한다. (??)\r\nDRN 은 dynamic feature selection과 refinement를 통해 방향성을 갖는 객체를 탐색한다. (??)\r\nCSL은 불연속적인 경계 문제를 피하기 위해 angular prediction 을 분류로 간주했다.\r\n최근에 몇몇 CenterNet 기반 방법은 작은 객체를 탐색하는데 이점을 보였다.\r\n위의 방법은 객체 표현 또는 feature 표현을 개선하는데 전념한다.\r\n반면에 우리의 방법은 backbone 부터 detection head 까지 feature 표현을 개선하는데 전념합니다.\r\n특리 우리의 방법은 backbone 에서 rotation-equivariant features 을 만든다.\r\n그것은 방향 변화를 모델링하는 복잡성을 상당히 감소시킨다.\r\ndetection head 에서 RiRoI Align 은 robust object localization 을 위해 rotation-invariant features 를 완벽히 추출한다.</p>\n<blockquote>\n<p>방향성을 가진 객체 탐색은 이렇게 발전해 왔고 우리의 방법은 여기서 한단계 더 나아갔다.</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>HBB, OBB</li>\n<li>aspect ratio</li>\n<li>anchor</li>\n<li>Gliding vertex</li>\n<li>CenterMap</li>\n<li>R3Det</li>\n<li>S2A-Net</li>\n<li>horizontal receptive fields</li>\n<li>rotated anchor</li>\n<li>DRN</li>\n<li>dynamic feature selection</li>\n<li>dynamic feature refinement</li>\n<li>CSL</li>\n<li>angular prediction</li>\n<li>CenterNet</li>\n<li>detection head</li>\n<li>robust object localization</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-rotation-equivariant-networks\" style=\"position:relative;\"><a href=\"#2-rotation-equivariant-networks\" aria-label=\"2 rotation equivariant networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(2) Rotation-equivariant Networks</h3>\n<p>4-fold rotation equivariance 를 CNN 에 통합시키기 위해 group convolution 이 제안됐다.(??)\r\nHexaConv 는 group convolution 을 6-fold rotation equivariance 로 확장시켰다.\r\n더 많은 방향에서 rotation equivariance 을 얻기 위해 몇몇 방법은 interpolation\r\n로 필터를 리셈플링한다. (??)\r\n다른 방법들은 연속적인 도메인에서 equivariant features 을 만들기 위한 harmonics 을 필터로 사용한다. (??)\r\n위의 방법은 점진적으로 더 큰 그룹으로 rotation equivariance 을 확장시켜서 classification 에서 유망한 결과를 얻었다.\r\n반면 우리의 방법은 rotation-equivariant networks 을 객체 탐색기에 통합하여 detection 에서 유망한 결과를 얻었다.\r\nrotation equivariance가 시스템적으로 방향성을 갖는 객체 탐색에 적용된 것은 처음이다.</p>\n<blockquote>\n<p>rotation equivariance 를 다른데서도 썼는데 객체 탐색에 방향성을 갖는 객체 탐색에 적용되는 것은 우리가 처음이다.</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>group convolution</li>\n<li>4-fold rotation</li>\n<li>rotation equivariance</li>\n<li>HexaConv</li>\n<li>interpolation</li>\n<li>harmonics</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-rotation-invariant-object-detection\" style=\"position:relative;\"><a href=\"#3-rotation-invariant-object-detection\" aria-label=\"3 rotation invariant object detection permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(3) Rotation-invariant Object Detection</h3>\n<p>rotation-invariant feature 는 임의의 방향성을 갖는 객체를 탐색하는데 중요하다.\r\n하지만 CNN 은 rotation variations 를 모델링 잘 못한다.\r\nCNN 은 방향정보를 해석하기 위해 더 많은 parameter 가 있어야한다.\r\nSTN 과 DCN 은 명백하게 network 안 rotation 을 모델로 만든다.\r\n그리고 방향성을 가진 객체에 적용돼왔다.\r\nCheng 은 정규화 강제성을 부여하는 rotation invariant layer 을 제시했다.\r\n위의 방법들이 이미지 레벨에서 대량의 rotation invariance 를 얻을지라도 많은 트레이닝 샘플과 parameter 가 필요하다.\r\n게다가 객체 탐색은 instance level rotation-invariant feature 가 필요하다.\r\n그러므로 몇몇 방법은 RoI wrapping 을 RRoI wrapping 으로 확장시킨다.\r\nRoI Transformer 은 HRoIs 를 RRoI 로 변화시키는 것을 배우고 region feature 을 rotated position sensitive RoI Align 로 감싼다.\r\n하지만 일반적인 CNN 은 rotation equivariant 와는 다르다.\r\n그러므로 심지어 RRoI Align 을 통해서도 우리는 rotation-invariant features 을 얻을 수 없다.\r\naforementioned 방법과는 다르게 우리의 방법은 s Rotation-invariant RoI Align (RiRoI Align) 을 제시한다.\r\n이것은 rotation-equivariant features 로부터 rotation-invariant features 을 얻을 수 있다.\r\n우리는 rotation-equivariant features 를 만들기 위해 rotation-equivariant networks 를 backbone 에 통합시킨다.\r\n그러면 RiRoI Align 은 완전하게 rotation-equivariant features 로 부터 rotationinvariant features 을 추출한다.</p>\n<blockquote>\n<p>일단 rotation-equivariant networks 를 backbone 에 통합시켜서 rotation-equivariant features 를 얻고 RiRoI Align 으로 rotation invariant features 를 얻어낸다.</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>STN</li>\n<li>DCN</li>\n<li>image level</li>\n<li>instance level</li>\n<li>aforementioned</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"preliminaries\" style=\"position:relative;\"><a href=\"#preliminaries\" aria-label=\"preliminaries permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Preliminaries</h2>\n<p>아 모르겠다.</p>\n<p>(1) equivariance\r\n(2) translation equivariance\r\n(3) the rotationequivariant convolution\r\n(4) the rotation transformation\r\n(5) the rotation equivariance\r\n(6) rotation-invariant feature</p>\n<blockquote>\n<p>rotation-invariant feature 를 얻기 위한 공식 유도</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>group action</li>\n<li>translation equivalriant</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"rotation-equivariant-detector\" style=\"position:relative;\"><a href=\"#rotation-equivariant-detector\" aria-label=\"rotation equivariant detector permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Rotation-equivariant Detector</h2>\n<p>rotation equivariance 와 rotation invariance 를 해석하기 위한 ReDet 의 세부사항에 대해서 말해주겠다\r\n우선  rotation equivariant networks 를 backbone 에 적용시켰다.\r\n전에 말했듯이 직접적으로 RRoI Aligh 을 rotation-equivariant feature maps 에 적용하는 것은 rotation-invariant features 를 얻을 수 없다.\r\n그러므로 novel Rotation-invariant RoI Align (RiRoI Align) 을 설계했다.\r\nrotationequivariant feature maps 으로 RoI-wise rotation-invariant features 를 얻을 수 있습니다.\r\n전반적인 아키텍쳐는 Fig.3 입니다.\r\n한 인풋 이미지를 rotation-equivariant backbone 에 준다.\r\n그리고 HRoIs 를 만들기 위해 RPN 채택한다.\r\nRPN 은 HRoIs 를 RRoIs 로 바꾸는 RoI Transformer 를 따른다. (??)\r\n마침내 RiRoI Align 은 rotation-invariant features 을 추출하기 위해 채택됐다\r\nRoI-wise classification 와 bounding box regression 을 위해</p>\n<blockquote>\n<p>아는게 없어서 계속 같은 말 하는 것 처럼 느껴진다.</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>HRoIs</li>\n<li>RoI Transformer</li>\n<li>RPN</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-rotation-equivariant-backbone\" style=\"position:relative;\"><a href=\"#1-rotation-equivariant-backbone\" aria-label=\"1 rotation equivariant backbone permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(1) Rotation-equivariant Backbone</h3>\n<p>현대 객체 탐색은 종종 semantic information 을 갖는 deep features 을 자동적으로 추출하기 위해 deep CNN 을 backbone 으로 채택한다.\r\n예를 들어 ResNet with Feature Pyramid Network (FPN)\r\n우리 또한 ResNet을 시작점으로 채택하고  rotation-equivariant backbone 을 구현했다.\r\nRotationequivariant ResNet 인 ReResNet 라고 부른다.\r\n우리는 rotation-equivariant networks 로 backbone 의 모든 레이어를 재구현했다.\r\n그 네트워크는 convolution, pooling, nomalization, non linearlities 를 포함하는 e2cnn 에 기반해서 이루어져있다.\r\ncomputational budget 를 고려했을 때 ReResNet 와 ReFPN 은 오직 discrete group 에만 등변한다.\r\nFig 3 에서 보이는 것처럼 우리는 rotation-equivariant feature maps 을 만들기 위해 rotation-equivariant backbone 에 이미지를 줬다.\r\n(??)\r\n평범한 backbone 과 비교했을 때, rotation equivariant backbone 은 다음과 같은 이점이 있다.\r\n더 높은 가중치의 공유.\r\nrotation-equivariant feature maps 는 부가적인 방향 차원을 갖고있다.\r\n다양한 방향으로부터의 feature 들은 종종 다른 rotation transformation 과 같은 필터를 공유한다.\r\n풍부한 방향 정보.\r\n고정된 방향을 갖는 인풋 이미지들에게 rotation-equivariant backbone 은 다양한 방향으로 부터의 feature 를 만들 수 있다.\r\n이것은 정확한 방향 정보가 필요한 방향성을 갖는 객체 탐색에 중요하다.\r\n작은 모델 사이즈.\r\nbaseline과 비교해서 우리는 backbone 을 설계할 때 두 가지 선택지가 있었다.\r\n비슷한 computation 이냐 비슷한 parameters 냐 하는.\r\n우리는 baseline 과 비슷한 computation 을 유지했다.\r\n예를 들어 같은 output channel 를 지키는.\r\nthe rotation weight sharing 때문에 우리의 rotation-equivariant backbone 은 모델 사이즈를 대략 1/N 정도로 엄청나게 감소 시킬 수 있었다.</p>\n<blockquote>\n<p>Rotation-equivariant Backbone 어떻게 만들었는지 그리고 그 효과</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>semantic information</li>\n<li>ResNet with Feature Pyramid Network (FPN)</li>\n<li>e2cnn</li>\n<li>convolution, pooling, nomalization, non linearlities</li>\n<li>computational budget</li>\n<li>discrete</li>\n<li>output channel</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2--rotation-invariant-roi-align\" style=\"position:relative;\"><a href=\"#2--rotation-invariant-roi-align\" aria-label=\"2  rotation invariant roi align permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(2)  Rotation-invariant RoI Align</h3>\n<p>Sec. 3 에 소개된 것처럼 우리는 RRoI warping 으로 감싸진 rotation equivariant feature maps 로부터 rotation-invariant RoI features 를 얻을 수 있었다.\r\n하지만 일반적인 RRoI wrapping 은 spartial dimension 에서 feature 들을 정렬시킬 수 없다.\r\n그러므로 우리는 완전하게 rotation-invariant features 를 얻기 위해 RiRoI Align 을 제시한다.\r\nFig 3 에 보여지는 것 처럼 RiRoI Align 는 두 부분으로 나뉜다.\r\nSpatial alignment.\r\nspatial alignment 는 rotation-invariant\r\nregion features fR 를 만들기위해 feature maps f 로부터 감싼다.\r\nOrientation alignment.\r\n다양한 방향에서의 RRoIs 가 완전하게 rotation-invariant features 를 만든다는 것을 보장하기 위해 우리는 방향 차원에서 방향 정렬을 수행한다.\r\n특히 output region feature 에서, 우리는 방향 정렬을 공식화했다.\r\n(공식)\r\n(???)</p>\n<blockquote>\n<p>공식을 이용한 유도</p>\n</blockquote>\n<p>RRoI Align+MaxPool 와의 비교.\r\nRoI features 를 감싸고 방향 차원을 maxpooling 하는 rotationinvariant features 을 얻는 다른 접근 법이다.\r\norientation pooling operation 은 종종 classification 일을 하는 데에 사용된다.\r\nfeature map 의 각 location 에서 이것은 오직 가장 강한 응답을 하는 방향만 보존한다.\r\n하지만 모든 방향으로부터의 응답은 객체 인식에 필수적이다.\r\n우리의 RiRoI Align에서 모든 방향으로부터의 feature 는 보존되고 정렬된다.\r\n우리는 RiRoI Align 의 이점을 보여주기 위해 실험을 진행했다. (Sec 5)</p>\n<blockquote>\n<p>다른 거는 가장 반응이 강한 방향만 사용하는데 우리는 모든 방향 사용한다.</p>\n</blockquote>\n<ul>\n<li>몰라요\n<ul>\n<li>orientation alignment in orientation dimension</li>\n<li>maxpooling</li>\n<li>orientation pooling operation</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"experiments-and-analysis\" style=\"position:relative;\"><a href=\"#experiments-and-analysis\" aria-label=\"experiments and analysis permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments and Analysis</h2>\n<h3 id=\"1-datasets\" style=\"position:relative;\"><a href=\"#1-datasets\" aria-label=\"1 datasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(1) Datasets</h3>\n<p>DOTA 는 항공 이미지에서 방향성을 갖는 객체 탐색을 위한 가장 큰 데이타셋이다.\r\nDOTA-v1.0 과 DOTA-v1.5 두 버전이 있다.\r\nDOTA-v1.0 은 2806 장의 항공 이미지\r\n사이즈 800x800 ~ 4000x4000\r\ninstance 188,282 개\r\n15개의 common category (plane, baseball diamond, bridge, ...)\r\nDOTA-v1.5 는 새로운 카테고리와 함께 DOAI Challenge 2019 때문에 나왔다.\r\n더 극도로 작은 instance - 402,089 개의 instance\r\nDOTA-v1.5 가 더 challenging 하지만 트레이닝 동안에 더 안정돼다.\r\ntraining 을 위해 training 과 validation set 둘 다를 썼다.\r\ntesting 을 위해 test set 를 썼다.\r\n우리는 원래 이미지를 a stride of 824 를 가진 1024 x 1024 patch 로 잘랐다.\r\nRandom horizontal flipping 은 over fitting 을 피하기 위해 사용됐다.\r\n또 다른 trick 은 이용되지 않았다.\r\n다른 방법들과 공평한 비교를 위해 우리는 세 scale 에서 multi scale 데이터를 준비했다.\r\n그리고 testing 과 trainging 을 위한 random rotation 을 준비했다\r\nHRSC2016 는 challenging 배 데이터셋이다.\r\nOBB annotations 가지고 있다.\r\n1061 개의 항공 이미지.\r\n사이즈는 300x300 ~ 1500x900\r\ntraining 436 개, validation 181 개 그리고 test 444 개의 이미지가 있다.\r\ntraining 을 위해 training 과 validation set 사용\r\ntesting 을 위해 testing set 사용\r\n모든 이미지는 aspect ratio 변화 없이 800 x 512 로 리사이징 됐다.\r\nRandom horizontal flipping 이 트레이닝 동안 적용됐다.</p>\n<ul>\n<li>몰라요\n<ul>\n<li>multi scale 데이터</li>\n<li>instance</li>\n<li>validation</li>\n<li>stride</li>\n<li>Random horizontal flipping</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-implementation-details\" style=\"position:relative;\"><a href=\"#2-implementation-details\" aria-label=\"2 implementation details permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(2) Implementation Details</h3>\n<p>ImageNet pretrain.\r\n보통의 ResNet 에서, 우리는 pytorch 로 직접적으로 ImageNet pretrained 모델을 사용한다.\r\nReResNet 에서, 우리는 mmclassification 을 기반으로 구현했다.\r\n우리는 초기 learning rate 0.1 로 ImageNet-1K 에서 ReResNet 을 훈련했다.\r\n모든 모델은 100 epochs 로 훈련됐다.\r\nlearning rate 는 30, 60, 90 epoch 에서 10으로 나눠졌다.\r\nbatch size 는 256 으로 설정됐다.</p>\n<p>Fine-tuning on detection.\r\n우리는 기본 method 의 backbone 으로 FPN 과 함께 ResNet 를 선택했다.\r\nResNet 과 함께하는 ReResNet 는 우리가 제시한 ReDet 의 backbone 으로 선택됐다.\r\nRPN 을 위해 우리는 피라미드 레벨의 각 location 마다 15 anchor 를 설정했다.\r\nR-CNN 을 위해 우리는 트레이닝을 위한 1:3 positive to negative ratio 로 512 RoIs 를 샘플로 만들었다.\r\n테스팅을 위해 우리는 NMS 전에 10000 RoIs 를 그리고 NMS 후에 2000 RoIs 를 선택했다.\r\n우리는 mmdetection 과 같은 트레이닝 스케쥴을 선택했다.\r\nSGD optimizer 는 초기 learning rate 0.01 로 선택됐다.\r\nlearning rate 는 매 decay 단계 마다 10으로 나뉘어졌다.\r\nmomentum 은 0.9 이다.\r\nweight edcay 는 0.0001 이다.\r\n우리는 모든 모델을 DOTA 에는 12 epochs 로 HRSC2016 는 36 epochs 로 훈련시켰다.\r\n우리는 트레이닝을 위해 total batch size 8 로 4V100 GPUs 를 사용했다.\r\ninference 를 위해 single V100 GPU 를 사용했다.</p>\n<ul>\n<li>몰라요\n<ul>\n<li>ImageNet pretrain</li>\n<li>epoch</li>\n<li>RPN</li>\n<li>R-CNN</li>\n<li>positive to negative ratio</li>\n<li>mmdetection</li>\n<li>decay step</li>\n<li>inference</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-ablation-studies\" style=\"position:relative;\"><a href=\"#3-ablation-studies\" aria-label=\"3 ablation studies permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(3) Ablation Studies</h3>\n<p>우리는 우리가 제시한 방법의 효율성을 평가하기 위해 DOTA-v1.5 test set 에서 일련의 ablation experiments.\r\n우리가 일바넞ㄱ인 ResNet+FPN, backbone 으로 RRoI Align, 기본 method 로 RoI warping method 를 사용했단 것에 주목해라.\r\nRotation-equivariant backbone.\r\n우리는 다른 세팅아래에 ReResNet50+ReFPN 로 rotation-equivariant backbone 의 효율성을 평가했다.\r\nTab 1 에서 보여지는 것처럼, ResNet50 과 비교했을 때 ResNet50 은 parameter 감소 때문에 더 낮은 classification accuracy 를 갖는다.\r\n하지만 더 높은 detection mAP 를 얻는다.\r\n우리는 cyclic group C8 아래에 있는 backbone 이 더 나은 accuracy-parameter-trade-off 를 얻는다는 거을 알아냈다.\r\n게다가 우리는 Tab 2 에서처럼 ReResNet+ReFPN 를 우리의 방법을 확장시켰다.\r\nFaster R-CNN OBB 와 ReResNet50+ReFPN 와 함께한 RetinaNet OBB 둘 다 이거의 counterpart 를 능가했다.</p>\n<p>Effectiveness of RiRoI Align.\r\nTab 3 에서 보이는 것처럼 RRoI Align 와 비교했을 때, RiRoI Align 은 방향 정렬 메카니즘 때문에 상당한 개선을 보인다.\r\nRRoI Align+MaxPool 이 mAP 에서 상당히 떨어졌기 때문에 orientation pooling 이 방향성을 갖는 객체 탐색에 바람지하기 못하단걸 알 수 있다.\r\nl = 2 interpolation 으로 한 RiRoI Align 은 가장 높은 66.86 mAP 을 얻었고 RRoI Align 보다 0.87 mAP 더 개선됐다.\r\n게다가 l = 4 interpolation 으로 한 RiRoI Align 이 오직 0.33 mAP 만을 얻었다는 것을 알아냈다.\r\n그 이유는 아마 너무 많은 interpolations 가 equivariant property 와 안에 방향 사이에 relation 을 망쳤기 때문일 것이다.</p>\n<p>Comparison with rotation augmentation.\r\n다른 관점에서, 우리의 방법은 한 방향에서 학습하면 다른 방향에 적용할 수 있는 특별한 in-network rotation augmentation 으로 보여질 수 있다.\r\n대조적으로 rotation augmentation 은 더 많은 방향을 갖는 샘플을 만들어서 network 를 강화시킨다.\r\n그리고 종종 더 많은 시간을 필요로 한다.\r\nTab 4 에서 보이는 것처럼 비록 우리의 방법이 1x schedule 아래 rotation augmented 된 baseline 에 지나지 않을 지라도, 우리의 ReDet (비슷한 양의 parameter 를 보존하는) 는 오직 18% 추가 training time 으로 2.59mAP 개선을 보여줬다.\r\n게다가 rotation augmentation 을 한 2xbaseline 은 ReDet 보다 0.68 더 높다.\r\n하지만 이것은 두 배의 시간을 필요로 한다.</p>\n<p>Performance on other datasets.\r\n우리가 제시한 방법의 일반화를 증명하기 위해 우리는 ReDet on DOTA-v1.0 and HRSC2016 의 수행을 측정했다.\r\nTab5 에서 보이는 것처럼 baseline 과 비교해서 ReDet 은 양쪽에서 더 나은 performance 를 보인다.\r\n게다가 ReDet 은 AP75 와 mAP 에서 상당히 개선됐다.</p>\n<ul>\n<li>몰라요\n<ul>\n<li>cyclic group</li>\n<li>mAP</li>\n<li>interpolations</li>\n<li>inner relation between orientations</li>\n<li>in-network rotation augmentation</li>\n<li>localization capabilities.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-comparisons-with-the-state-of-the-art\" style=\"position:relative;\"><a href=\"#4-comparisons-with-the-state-of-the-art\" aria-label=\"4 comparisons with the state of the art permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(4) Comparisons with the State-of-the-Art</h3>\n<p>Results on DOTA-v1.0.\r\nTab6 에서 보이는 것처럼 우리는 우리의 ReDet 을 DOTA-v1.0 OBB Task 상에서 다른 좋은 방법들과 비교했다.\r\n벨과 호루라기 없이 우리의 signle-scale model 은 76.25mAP 를 얻었다.\r\n모든 single-scale model 을 능가하고 대부분의 multi-scale models 능가한다.\r\n제한된 데이터 augmentation 으로 우리의 방법은 모든 데이터셋에서 80.10 mAP 를 얻었고 12/15 category 중 1, 2등을 했다.</p>\n<p>Results on DOTA-v1.5.\r\nDOTA-v1.0 과 비교했을 때 DOTA-v1.5 는 객체 탐색에 어려움을 증가시키는 많은 극도로 작은 instance 를 포함한다.\r\n우리는 Tab 7 에서 DOTA-v1.5 에 대한 OBB, HBB 결과를 보고했다.\r\nsignle-scale data 에서 우리의 방법은 66.86 oBB mAP 오 67.77 HBB mAP 를 얻었다.\r\nRetinaNet OBB, Faster R-CNN OBB, Mask R-CNN and HTC 를 능가한다.\r\n특히 small instance 와 큰 scale variation 을 갖는 category 에서 우리의 방법은 더 잘 수행된다.\r\n게다가 Fig 2 에서 보이는 것처럼 우리의 ReDet 은 더 나은 parameter vs accuracy trade off 를 얻었다.\r\n이전 OWSR 에 의한 최고의 결과와 비교했을 때, 우리의 multi-scale model 은 최고의 성능을 달성했다.\r\n대략 76.80 oBB mAP 와 78.08 HBB mAP.\r\nReDet 과 baseline 과의 질적 비교는 Fig 4 에 있다.</p>\n<p>Result on HRSC2016.\r\nHRSC2016 는 많은 얇고 긴 배 instance 를 포함한다.\r\n우리는 Tab 8 에서 RedDet 과 다른 최신 방법과 비교했다.\r\n우리의 방법은 최신 성능을 달성했다.\r\nVOC2007, VOC2012 아래에서 각각 90.46 mAP 와 97.63 mAP</p>\n<ul>\n<li>몰라요\n<ul>\n<li>signle-scale model, multi-scale models</li>\n<li>VOC2007, VOC2012</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"conclusions\" style=\"position:relative;\"><a href=\"#conclusions\" aria-label=\"conclusions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusions</h2>\n<p>ReDet 두 부분으로 이루어져있다.</p>\n<ol>\n<li>rotation-equivariant backbone</li>\n<li>RiRoI Align</li>\n</ol>\n<p>전자는 rotation-equivariant features 만든다\r\n후자는 rotation-equivariant features 로 rotation-invariant features 얻는다.</p>\n<p>DOTA 와 HRSC2016 에서 진행한 광범위한 실험은 이 방법이 효과적임을 보인다.</p>"}}},"staticQueryHashes":[],"slicesMap":{}}