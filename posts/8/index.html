<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 5.4.2"/><meta data-react-helmet="true" name="description" content="2021년 3월 13일에 나온 따끈따끈한 논문 해석을 해보겠습니다. 논문을 이번에 처음 읽어봤고 아는게 많이 없어서 이해 안된 부분이 많고 그런 부분은 메모를 해놨다. 뭔 말인지 진짜 모르겠다."/><meta data-react-helmet="true" name="keywords" content="blog,sunrisehouse,jungwoo han,한정우,블로그,programming,software,coding,프로그래밍,코딩,개발,developer"/><meta data-react-helmet="true" name="author" content="Jungwoo Han | sunrisehouse"/><meta data-react-helmet="true" name="og:image" content="https://sunrisehouse.github.io/images/hanjws.jpg"/><meta data-react-helmet="true" property="og:site_name" content="Jungwoo Han Blog"/><meta data-react-helmet="true" property="og:title" content="ReDet 논문 해석"/><meta data-react-helmet="true" property="og:description" content="2021년 3월 13일에 나온 따끈따끈한 논문 해석을 해보겠습니다. 논문을 이번에 처음 읽어봤고 아는게 많이 없어서 이해 안된 부분이 많고 그런 부분은 메모를 해놨다. 뭔 말인지 진짜 모르겠다."/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:creator" content="Jungwoo Han | sunrisehouse"/><meta data-react-helmet="true" name="twitter:title" content="ReDet 논문 해석"/><meta data-react-helmet="true" name="twitter:description" content="2021년 3월 13일에 나온 따끈따끈한 논문 해석을 해보겠습니다. 논문을 이번에 처음 읽어봤고 아는게 많이 없어서 이해 안된 부분이 많고 그런 부분은 메모를 해놨다. 뭔 말인지 진짜 모르겠다."/><meta data-react-helmet="true" name="google-site-verification" content="RZnFgzTyqGnyCqfaCKx2-HloYzNA3D3yrC2N1-J1K5U"/><meta data-react-helmet="true" name="naver-site-verification" content="189cca7bf40ac78f41ef4578ab1084d6a017fca2"/><style data-href="/styles.e93c99669b582638bf6a.css" data-identity="gatsby-global-css">@import url(//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSansNeo.css);*{margin:0;padding:0}body{color:#212121}body,input,textarea{font:400 18px/1.8 Spoqa Han Sans Neo,Apple SD Gothic Neo,Nanum Barun Gothic,Nanum Gothic,Verdana,Arial,Malgun Gothic,Dotum,sans-serif}a{outline:none}a,a:active,a:hover{color:#212121;text-decoration:none}a:active,a:hover{background-color:transparent}button{-webkit-appearance:none;appearance:none}button:focus{box-shadow:none;outline:0}*{-webkit-tap-highlight-color:rgba(0,0,0,0)}code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#000;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;hyphens:none;line-height:1.5;tab-size:4;text-align:left;text-shadow:0 1px #fff;white-space:pre;word-break:normal;word-spacing:normal}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:#b3d4fc;text-shadow:none}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{background:hsla(0,0%,100%,.5);color:#9a6e3a}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.gm-page-comp{background-color:#000}.gm-page-comp>.page-center-container{margin:0 auto;max-width:1200px;padding:20px}@media(min-width:1000px){.gm-page-comp>.page-center-container{padding:40px}}.gm-page-comp>.page-center-container>.title{color:#fff;padding:100px;text-align:center}.gm-page-comp>.page-center-container>.nft-container{background-color:#fff;height:1500px;overflow:hidden;position:relative}.gm-page-comp>.page-center-container>.nft-container>.contents-container{height:2220px;left:0;position:absolute;top:-720px;width:100%}@media(min-width:410px){.gm-page-comp>.page-center-container>.nft-container>.contents-container{height:2140px;top:-640px}}@media(min-width:1000px){.gm-page-comp>.page-center-container>.nft-container>.contents-container{height:2040px;top:-540px}}.gm-page-comp>.page-center-container>.nft-container>.loading{animation:fadein 1s;font-size:40px;height:0;left:50%;opacity:0;position:absolute;top:20%;transform:translate(-50%,-50%)}@keyframes fadein{0%{opacity:0}to{opacity:1}}.border-button-comp{color:#666;cursor:pointer;display:block;font-size:16px;font-weight:600;margin:0;padding:16px;text-align:center;transition:background-color .3s;width:100%}.border-button-comp,.border-button-comp:active{background-color:#fff;border:1px solid #f0f0f0}@media(min-width:1000px){.border-button-comp:hover{background-color:#f0f0f0;border:1px solid #f0f0f0}}.post-card-comp{background-color:#fff;border-radius:24px;box-shadow:0 2px 12px 0 rgba(0,0,0,.5);cursor:pointer;overflow:hidden;transition:box-shadow .5s,transform .5s}@media(min-width:1000px){.post-card-comp{box-shadow:0 2px 16px 0 rgba(0,0,0,.5)}}.post-card-comp:hover{transform:scale(1.04)}.post-card-comp:hover>.main-image-container>.main-image{transform:scale(1)}.post-card-comp>.main-image-container{background-color:#f0f0f0;height:120px;overflow:hidden;position:relative}.post-card-comp>.main-image-container>.main-image{background-image:url(/images/hanjws.jpg);background-position:50%;background-repeat:no-repeat;background-size:cover;height:100%;transform:scale(1.4);transition:transform .5s;width:100%}.post-card-comp>.main-image-container>.curtain{background-color:rgba(0,0,0,.2);height:100%;left:0;position:absolute;top:0;width:100%}.post-card-comp>.description-container{padding:16px}.post-card-comp>.description-container>.date{color:#999;font-size:12px;line-height:1.4}.post-card-comp>.description-container>.title{-webkit-line-clamp:1;-webkit-box-orient:vertical;display:-webkit-box;font-size:20px;font-weight:600;height:32px;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:normal;word-break:keep-all}@media(min-width:1000px){.post-card-comp>.description-container>.title{-webkit-line-clamp:1;font-size:20px;height:36px}}.post-card-comp>.description-container>.post-theme-container{line-height:1.5;margin-bottom:16px}.post-card-comp>.description-container>.post-theme-container>span{border:1px solid #f0f0f0;border-radius:4px;color:#999;font-size:12px;margin-right:2px;padding:2px 4px}.post-card-comp>.description-container>.description{-webkit-line-clamp:2;-webkit-box-orient:vertical;color:#666;display:-webkit-box;font-size:16px;font-weight:400;line-height:1.4;min-height:48px;overflow:hidden;text-overflow:ellipsis;white-space:normal;word-break:keep-all}.home-page-comp{padding-bottom:60px}.home-page-comp .centered-container{box-sizing:border-box;margin:0 auto;max-width:1000px;width:100%}.home-page-comp>.title{font-size:20px;font-weight:500;padding:20px 16px}@media(min-width:1000px){.home-page-comp>.title{font-size:24px}}.home-page-comp>.greeting{align-items:center;animation:appear 1s;animation-timing-function:linear;display:flex;height:300px;justify-content:center;padding-bottom:80px;position:relative}@media(min-width:1000px){.home-page-comp>.greeting{height:500px}}.home-page-comp>.greeting>canvas{bottom:0;height:100%;left:0;position:absolute;width:100%}.home-page-comp>.post-container{animation:appear2 2s;animation-timing-function:linear;padding:20px}.home-page-comp>.post-container>.post-wrapper{animation:appear 1s;margin-bottom:28px}.home-page-comp>.post-container>.post-wrapper:last-child{margin-bottom:0}.home-page-comp>.more-button-container{margin-top:20px;padding:0 16px}.comment-box-comp{color:#666;font-size:16px;padding:32px 0 80px}.comment-box-comp .writer-container{align-items:baseline;display:flex}.comment-box-comp .writer-container .name{font-weight:600}.comment-box-comp .writer-container .date{font-size:14px;margin-left:20px}.register-comment-form-comp{font-size:16px}.register-comment-form-comp .caution{background-color:#eef;color:#66f;font-size:14px;margin-top:20px;padding:12px}.register-comment-form-comp .form{box-shadow:0 2px 4px 0 rgba(0,0,0,.5);padding:20px}.register-comment-form-comp .writer-label{display:flex;font-size:600}.register-comment-form-comp .writer-label .label{padding:8px 0;width:80px}.register-comment-form-comp .writer-label>input{flex:1 1;font-size:14px;padding:8px 12px}.register-comment-form-comp .content-label{display:flex;margin-top:20px}.register-comment-form-comp .content-label .label{padding:8px 0;width:80px}.register-comment-form-comp .content-label>textarea{box-sizing:border-box;flex:1 1;font-size:14px;padding:8px 12px}.register-comment-form-comp .submit-button-container{margin-top:20px;text-align:right}.register-comment-form-comp .submit-button-container .submit-button{background-color:#66f;border:none;border-radius:4px;color:#fff;cursor:pointer;font-weight:600;padding:12px;transition:background-color .4s}.register-comment-form-comp .submit-button-container .submit-button:hover{background-color:#eef;color:#444}.markdown-style>*{margin-bottom:12px}.markdown-style ol,.markdown-style ul{padding-left:20px}.markdown-style pre{font-size:14px}.markdown-style blockquote{background-color:#f0f0f0;border-left:4px solid #4765ff;padding:20px 16px}.markdown-style hr{margin-top:65px}.markdown-style a{color:#212121;outline:none;text-decoration:underline}.markdown-style a:active,.markdown-style a:hover{background-color:#4765ff;color:#fff;text-decoration:underline}.markdown-style h1{margin-top:64px}.markdown-style h2{margin-top:60px}.markdown-style h3{margin-top:56px}.markdown-style h4{margin-top:52px}.markdown-style h5{margin-top:48px}.markdown-style h6{margin-top:44px}.markdown-style h1{font-size:28px}@media(min-width:1000px){.markdown-style h1{font-size:36px}}.markdown-style h2{font-size:22px}@media(min-width:1000px){.markdown-style h2{font-size:26px}}.markdown-style h3{font-size:20px}@media(min-width:1000px){.markdown-style h3{font-size:22px}}.markdown-style pre{font-size:16px}.markdown-style li,.markdown-style p{color:#444;font-size:16px;line-height:28px}.markdown-style table{background-color:#eee;border-radius:4px;box-shadow:0 2px 4px 0 rgba(0,0,0,.5)}.markdown-style table td,.markdown-style table th{border-bottom:1px solid #fff;font-size:16px;padding:16px 20px}.markdown-style .anchor.before{padding-left:0}@keyframes appear{0%{opacity:0}to{opacity:1}}@keyframes appear2{0%{opacity:0}50%{opacity:0}to{opacity:1}}.post-page-comp>.main-image-container{background-color:#f0f0f0;background-position:50%;background-repeat:no-repeat;background-size:cover;height:200px;position:relative}@media(min-width:1000px){.post-page-comp>.main-image-container{height:300px}}.post-page-comp>.main-image-container>.cover{background-color:rgba(0,0,0,.7);height:100%;left:0;position:absolute;right:0;width:100%}.post-page-comp>.main-image-container>.cover>.center{box-sizing:border-box;left:50%;padding:0 16px;position:absolute;top:50%;transform:translate(-50%,-50%);width:100%}.post-page-comp>.main-image-container>.cover>.center>.title{color:#fff;font-size:24px;font-weight:500;line-height:35px;text-align:center}@media(min-width:1000px){.post-page-comp>.main-image-container>.cover>.center>.title{font-size:40px;font-weight:500;line-height:60px}}.post-page-comp>.main-image-container>.cover>.center>.date{color:#fff;font-size:13px;font-weight:400;letter-spacing:-.02em;line-height:16px;margin-top:8px;text-align:center}.post-page-comp>.main-image-container>.cover>.left-bottom{bottom:20px;left:16px;position:absolute}.post-page-comp>.main-image-container>.cover>.left-bottom>.date{color:#fff;font-size:13px;font-weight:400;letter-spacing:-.02em;line-height:16px}.post-page-comp>.contents-container{margin-bottom:80px;overflow:hidden;padding:20px}@media(min-width:1000px){.post-page-comp>.contents-container{margin:0 auto 100px;max-width:800px;width:100%}}.post-page-comp>.contents-container>.description{animation:appear 1s;animation-timing-function:linear}.post-page-comp>.contents-container>.comments-section{border-top:2px solid #666;margin-top:160px;padding-top:40px}.post-page-comp>.contents-container>.comments-section>h3{margin-bottom:40px}.post-page-comp>.contents-container>.comments-section>.comments-container{margin-top:40px}.post-page-comp>.contents-container>.comments-section>.comments-container>.comment-wrapper{border-bottom:1px solid #aaa}.post-page-comp>.home-button{left:16px;position:fixed;top:12px}@media(min-width:1000px){.post-page-comp>.home-button{left:24px;top:16px}}.post-page-comp>.home-button>.hanjungwoo{color:#333;font-weight:400;text-decoration:none}@media(min-width:1000px){.post-page-comp>.home-button>.hanjungwoo{font-size:28px}}.post-page-comp>.home-button>.hanjungwoo:hover{text-decoration:underline}.main-page{font-family:Noto Sans KR,sans-serif;line-height:1.5}.main-page .appbar-container{z-index:1}.main-page .image-section{background-image:url(https://decipher.ac/img/snu_background.png);background-position:50%,50%;background-repeat:none;background-size:cover;height:calc(100vh - 64px)}.main-page .image-section .text{box-sizing:border-box;color:#fff;font-size:36px;font-weight:700;margin:0 auto;max-width:1200px;padding-left:15px;padding-right:15px;padding-top:calc(50vh - 180px);width:100%}.main-page .strength-section{padding:100px 15px}.main-page .strength-section h2{font-size:24px;font-weight:600;margin-bottom:75px;text-align:center}@media(min-width:1000px){.main-page .strength-section h2{font-size:28px}}.main-page .strength-section .box-container .box{align-items:center;display:flex;flex-direction:column;padding:50px;text-align:center}.main-page .strength-section .box-container .box img{height:60px;margin-bottom:20px;width:60px}.main-page .strength-section .box-container .box h3{margin-bottom:20px}.main-page .strength-section .box-container .box p{font-size:16px;font-weight:300}@media(min-width:1000px){.main-page .strength-section .box-container{display:flex;justify-content:center;padding:60px 0}}</style><style type="text/css">
    .anchor.before {
      position: absolute;
      top: 0;
      left: 0;
      transform: translateX(-100%);
      padding-right: 4px;
    }
    .anchor.after {
      display: inline-block;
      padding-left: 4px;
    }
    h1 .anchor svg,
    h2 .anchor svg,
    h3 .anchor svg,
    h4 .anchor svg,
    h5 .anchor svg,
    h6 .anchor svg {
      visibility: hidden;
    }
    h1:hover .anchor svg,
    h2:hover .anchor svg,
    h3:hover .anchor svg,
    h4:hover .anchor svg,
    h5:hover .anchor svg,
    h6:hover .anchor svg,
    h1 .anchor:focus svg,
    h2 .anchor:focus svg,
    h3 .anchor:focus svg,
    h4 .anchor:focus svg,
    h5 .anchor:focus svg,
    h6 .anchor:focus svg {
      visibility: visible;
    }
  </style><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><title data-react-helmet="true">ReDet 논문 해석 | Jungwoo Han</title><link rel="alternate" type="application/rss+xml" title="sunrisehouse rss feed" href="/rss.xml"/><link rel="sitemap" type="application/xml" href="/sitemap-index.xml"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="post-page-comp"><div class="main-image-container" style="background-image:url()"><div class="cover"><div class="center"><h1 class="title">ReDet 논문 해석</h1><h3 class="date">2021-04-02</h3></div><div class="left-bottom"></div></div></div><div class="contents-container"><div class="description markdown-style"><h1 id="redet-논문-리뷰" style="position:relative;"><a href="#redet-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0" aria-label="redet 논문 리뷰 permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ReDet 논문 리뷰</h1>
<p>2021년 3월 13일에 나온 따끈따끈한 논문 해석을 해보겠습니다.
논문을 이번에 처음 읽어봤고 아는게 많이 없어서 이해 안된 부분이 많고 그런 부분은 메모를 해놨다.
뭔 말인지 진짜 모르겠다.</p>
<h2 id="redet" style="position:relative;"><a href="#redet" aria-label="redet permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Redet</h2>
<p><a href="https://arxiv.org/abs/2103.07733">링크</a></p>
<p>제목: ReDet: A Rotation-equivariant Detector for Aerial Object Detection</p>
<p>코드: <a href="https://github.com/csuhan/ReDet">깃허브 링크</a></p>
<p>연구 주제: Computer Vision and Pattern Recognition</p>
<h3 id="abstraction" style="position:relative;"><a href="#abstraction" aria-label="abstraction permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>abstraction</h3>
<p>자연 이미지에서와는 다르게 항공 이미지에서의 객체는 임의의 방향성을 갖는다.
그러므로 항공 객체의 방향을 해석하기 위해서는 더 많은 정보가 필요하다.
일반적인 CNN 은 방향 변화를 명시적으로 모델링하지 않으므로 많은 양의 회전 증강 데이터가 필요합니다. (이미지 회전 시켜서 데이터로 쓰는 작업을 말하는 것 같다.)
그러므로 이 논문에서는 회전 등변성 및 회전 불변성을 명시 적으로 해석하는 Rotation-equivariant성 Detetor (ReDet)를 제안합니다.
네트워크에 회전 등변 네트워크를 detector 에 통합해서 방향을 정확하게 예측하고 모델 크기를 줄입니다.
회전 등변 기능을 기반으로 RoI의 방향에 따라 등변 기능에서 회전 불변 기능을 적응 적으로 추출하는 Rotation-invariant RoI Align (RiRoI Align)도 제시합니다.
여러 도전적인 항공 이미지 데이터 세트 DOTA-v1.0, DOTA-v1.5 및 HRSC2016에 대한 광범위한 실험을 통해 우리의 방법이 항공 물체 감지 작업에서 최첨단 성능을 달성 할 수 있음을 보여줍니다.
이전 최고의 결과와 비교하여 ReDet은 DOTA-v1.0, DOTA-v1.5 및 HRSC2016에서 각각 1.2, 3.5 및 2.6mAP를 확보하면서 매개 변수 수를 60 % 줄였습니다 (313Mb 대 121Mb)</p>
<ul>
<li>몰라요
<ul>
<li>회전 불변성</li>
<li>RoI</li>
<li>회전 등변성</li>
</ul>
</li>
</ul>
<hr>
<h3 id="introduction" style="position:relative;"><a href="#introduction" aria-label="introduction permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h3>
<p>제약이없는 항공 이미지에서 정확한 물체 감지를 달성하기 위해 대부분은 회전 불변 특징을 추출하는 데 전념합니다.
실제로 Rotated RoI (RRoI) wrapping 은 rotation-invariant features 를 추출하는 데 가장 흔히 사용된다.
하지만 일반적인 CNN feature 들과 wrapping 한 RRoI 는 rotation-invariant feature 를 만들어 낼 수 없다.
rotation invariance 은 더 큰 수용 능력을 가진 network, 더 많은 training 샘플들로 만들어진다.
CNN에 회전된 이미지를 주는 것은 원래 이미지의 rotation feature map 과 다르다. (??)
region features warped from regular CNN feature 는 종종 불안정하고 방햔 변화에 민감하다.</p>
<blockquote>
<p>CNN 에 RRoI wrapping 한 것은 한계가 있다.</p>
</blockquote>
<p>최근에 나온 몇몇 방법은 CNN 을 더 큰 그룹으로 확장시키고 그룹 convolution 으로 rotation equivariance 를 얻는다.
이 방법의 feature map 은 부가적인 방향 channel 을 갖는다. (?? channel??)
하지만 보통의 rotation-equivariant features 에 쌓인 RRoI 를 직접 적용하는 것은 rotation-invariant features 를 만들지 못한다.
왜냐하면 orientation channels 는 여전히 어긋나도 2D 평면의 region features 만 감싸기 때문이다.
rotation-invariant features 을 정확하게 얻기 위해서 RRoI 의 방향에 따라서 orientation dimension of feature maps 을 조절해야한다.</p>
<blockquote>
<p>최근 나온 방법보다 더 좋은게 있는데 RRoI 방향에 따라 orientation dimension of feature maps 을 조절하는 것</p>
</blockquote>
<p>이 논문에서 rotation-equivariant features 을 통해 rotation-invariant features 을 정확하게 추출하기 위한 ReDet 을 제시한다.
이 논문은 rotation-equivariant feature 추출과 rotation-invariant feature 추출 두 부분으로 이루어져있다.
우선 rotation-equivariant features 추출을 위해서 rotation-equivariant networks 를 backbone 과 통합한다.
하지만 직접적으로 RRoI warping 을 적용하는 것은 여전히 rotation-equivariant features 로 부터 rotation-invariant features 을 구할 수 없다.
그러므로 novel Rotation-invariant RoI Align (RiRoI Align) 를 제시한다.
이것은 spartial dimension 안 RRoI 경계 박스에 따른 region feature 를 감싼다.
또 orientation channels 과 feature interpolation 을 바꾸면서 orientation dimesion 안 align features 도 감싼다. (??)</p>
<blockquote>
<p>더 정확학게 추출하기 위해서 우리는 ReDet 을 제시한다.</p>
</blockquote>
<p>항공 이미지 data set DOTA 로 광범위한 실험을 수행했다.
HRSC2016 우리 방법의 효율성을 증명했다.
우리는 고품질 항공 객체 탐색을 위한 rotation equivariance 와 rotation invariance 를 해석하는 Redet 을 제시한다.
rotation equivariance 가 방향을 갖는 객체 탐색에 도입된 것은 이번이 처음이다.
우리는 또한 rotation-equivariant features 로부터 rotation-invariant features 을 추출하기 위해서 novel RiRoI Align 를 설계했다.
다른 RRoI warping 방법과는 다르게 RiRoI Align 은 spatial and orientation dimensions 에서 완전하게 rotation-invariant features
우리의 방법은 DOTA-v1.0, DOTA-v1.5 및 HRSC2016에서 각각 최첨단 80.10, 76.80 및 90.46 mAP를 달성합니다. (??)
이전의 최상의 결과와 비교하여 우리의 방법은 1.2, 3.5 및 2.6 mAP 향상을 얻었습니다. (??)
기준과 비교하여 우리의 방법은 일관되고 실질적인 개선을 보여 주며 매개 변수 수를 60 % 감소시킵니다 (313Mb 대 121Mb). (??)
또한, 우리의 방법은 더 나은 모델 크기 대 정확도 절충을 달성합니다. (??)</p>
<blockquote>
<p>Redet 제시와 novel RiRoI Align 설계를 했고 우리의 방법은 효과적이었다.</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>회전 불변 feature (rotation invariant feature)</li>
<li>RRoI wrapping</li>
<li>convolution</li>
<li>Feature map</li>
<li>region feature</li>
<li>networks</li>
<li>backbone</li>
<li>spartial dimension</li>
<li>orientation channels</li>
<li>feature interpolation</li>
<li>align features</li>
</ul>
</li>
</ul>
<hr>
<h3 id="related-works" style="position:relative;"><a href="#related-works" aria-label="related works permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Related Works</h3>
<h4 id="1-oriented-object-detection" style="position:relative;"><a href="#1-oriented-object-detection" aria-label="1 oriented object detection permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(1) Oriented Object Detection</h4>
<p>HBB 를 사용하는 대부분의 일반적인 객체 탐색과는 다르게 방향성을 갖는 객체 탐색은 OBB 를 사용한다.
OBB 는 큰 aspect ratio 을 가진 항공물체를 탐색하는데 필수적이다.
일반적인 객체 탐색 개발과 함께 방향성을 가진 객체 탐색에도 잘 설계된 방법이 제시돼왔다
임의의 방향성을 갖는 객체 탐색을 하기 위해서 몇몇 방법은 많은 rotated anchors 를 채택했다.
많은 rotated anchors 는 계산을 복잡하게 하지만 다양한 각도, 크기, aspect ratio 를 얻을 수 있다.
Ding 은 많은 anchor 를 피하기 위해 Horizontal RoIs (HRoIs) 를 RRoIs 로 변형시키기 위한 RoI Transformer 를 제시했다.
Gliding vertex 와 CenterMap 은 각각 방향이있는 객체를 정확하게 설명하기 위해 사각형과 마스크를 사용합니다.
R3Det and S2A-Net 는 horizontal receptive fields 와 rotated anchor 사이에 feature 를 정렬한다. (??)
DRN 은 dynamic feature selection과 refinement를 통해 방향성을 갖는 객체를 탐색한다. (??)
CSL은 불연속적인 경계 문제를 피하기 위해 angular prediction 을 분류로 간주했다.
최근에 몇몇 CenterNet 기반 방법은 작은 객체를 탐색하는데 이점을 보였다.
위의 방법은 객체 표현 또는 feature 표현을 개선하는데 전념한다.
반면에 우리의 방법은 backbone 부터 detection head 까지 feature 표현을 개선하는데 전념합니다.
특리 우리의 방법은 backbone 에서 rotation-equivariant features 을 만든다.
그것은 방향 변화를 모델링하는 복잡성을 상당히 감소시킨다.
detection head 에서 RiRoI Align 은 robust object localization 을 위해 rotation-invariant features 를 완벽히 추출한다.</p>
<blockquote>
<p>방향성을 가진 객체 탐색은 이렇게 발전해 왔고 우리의 방법은 여기서 한단계 더 나아갔다.</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>HBB, OBB</li>
<li>aspect ratio</li>
<li>anchor</li>
<li>Gliding vertex</li>
<li>CenterMap</li>
<li>R3Det</li>
<li>S2A-Net</li>
<li>horizontal receptive fields</li>
<li>rotated anchor</li>
<li>DRN</li>
<li>dynamic feature selection</li>
<li>dynamic feature refinement</li>
<li>CSL</li>
<li>angular prediction</li>
<li>CenterNet</li>
<li>detection head</li>
<li>robust object localization</li>
</ul>
</li>
</ul>
<h3 id="2-rotation-equivariant-networks" style="position:relative;"><a href="#2-rotation-equivariant-networks" aria-label="2 rotation equivariant networks permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(2) Rotation-equivariant Networks</h3>
<p>4-fold rotation equivariance 를 CNN 에 통합시키기 위해 group convolution 이 제안됐다.(??)
HexaConv 는 group convolution 을 6-fold rotation equivariance 로 확장시켰다.
더 많은 방향에서 rotation equivariance 을 얻기 위해 몇몇 방법은 interpolation
로 필터를 리셈플링한다. (??)
다른 방법들은 연속적인 도메인에서 equivariant features 을 만들기 위한 harmonics 을 필터로 사용한다. (??)
위의 방법은 점진적으로 더 큰 그룹으로 rotation equivariance 을 확장시켜서 classification 에서 유망한 결과를 얻었다.
반면 우리의 방법은 rotation-equivariant networks 을 객체 탐색기에 통합하여 detection 에서 유망한 결과를 얻었다.
rotation equivariance가 시스템적으로 방향성을 갖는 객체 탐색에 적용된 것은 처음이다.</p>
<blockquote>
<p>rotation equivariance 를 다른데서도 썼는데 객체 탐색에 방향성을 갖는 객체 탐색에 적용되는 것은 우리가 처음이다.</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>group convolution</li>
<li>4-fold rotation</li>
<li>rotation equivariance</li>
<li>HexaConv</li>
<li>interpolation</li>
<li>harmonics</li>
</ul>
</li>
</ul>
<h3 id="3-rotation-invariant-object-detection" style="position:relative;"><a href="#3-rotation-invariant-object-detection" aria-label="3 rotation invariant object detection permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(3) Rotation-invariant Object Detection</h3>
<p>rotation-invariant feature 는 임의의 방향성을 갖는 객체를 탐색하는데 중요하다.
하지만 CNN 은 rotation variations 를 모델링 잘 못한다.
CNN 은 방향정보를 해석하기 위해 더 많은 parameter 가 있어야한다.
STN 과 DCN 은 명백하게 network 안 rotation 을 모델로 만든다.
그리고 방향성을 가진 객체에 적용돼왔다.
Cheng 은 정규화 강제성을 부여하는 rotation invariant layer 을 제시했다.
위의 방법들이 이미지 레벨에서 대량의 rotation invariance 를 얻을지라도 많은 트레이닝 샘플과 parameter 가 필요하다.
게다가 객체 탐색은 instance level rotation-invariant feature 가 필요하다.
그러므로 몇몇 방법은 RoI wrapping 을 RRoI wrapping 으로 확장시킨다.
RoI Transformer 은 HRoIs 를 RRoI 로 변화시키는 것을 배우고 region feature 을 rotated position sensitive RoI Align 로 감싼다.
하지만 일반적인 CNN 은 rotation equivariant 와는 다르다.
그러므로 심지어 RRoI Align 을 통해서도 우리는 rotation-invariant features 을 얻을 수 없다.
aforementioned 방법과는 다르게 우리의 방법은 s Rotation-invariant RoI Align (RiRoI Align) 을 제시한다.
이것은 rotation-equivariant features 로부터 rotation-invariant features 을 얻을 수 있다.
우리는 rotation-equivariant features 를 만들기 위해 rotation-equivariant networks 를 backbone 에 통합시킨다.
그러면 RiRoI Align 은 완전하게 rotation-equivariant features 로 부터 rotationinvariant features 을 추출한다.</p>
<blockquote>
<p>일단 rotation-equivariant networks 를 backbone 에 통합시켜서 rotation-equivariant features 를 얻고 RiRoI Align 으로 rotation invariant features 를 얻어낸다.</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>STN</li>
<li>DCN</li>
<li>image level</li>
<li>instance level</li>
<li>aforementioned</li>
</ul>
</li>
</ul>
<hr>
<h2 id="preliminaries" style="position:relative;"><a href="#preliminaries" aria-label="preliminaries permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Preliminaries</h2>
<p>아 모르겠다.</p>
<p>(1) equivariance
(2) translation equivariance
(3) the rotationequivariant convolution
(4) the rotation transformation
(5) the rotation equivariance
(6) rotation-invariant feature</p>
<blockquote>
<p>rotation-invariant feature 를 얻기 위한 공식 유도</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>group action</li>
<li>translation equivalriant</li>
</ul>
</li>
</ul>
<hr>
<h2 id="rotation-equivariant-detector" style="position:relative;"><a href="#rotation-equivariant-detector" aria-label="rotation equivariant detector permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Rotation-equivariant Detector</h2>
<p>rotation equivariance 와 rotation invariance 를 해석하기 위한 ReDet 의 세부사항에 대해서 말해주겠다
우선  rotation equivariant networks 를 backbone 에 적용시켰다.
전에 말했듯이 직접적으로 RRoI Aligh 을 rotation-equivariant feature maps 에 적용하는 것은 rotation-invariant features 를 얻을 수 없다.
그러므로 novel Rotation-invariant RoI Align (RiRoI Align) 을 설계했다.
rotationequivariant feature maps 으로 RoI-wise rotation-invariant features 를 얻을 수 있습니다.
전반적인 아키텍쳐는 Fig.3 입니다.
한 인풋 이미지를 rotation-equivariant backbone 에 준다.
그리고 HRoIs 를 만들기 위해 RPN 채택한다.
RPN 은 HRoIs 를 RRoIs 로 바꾸는 RoI Transformer 를 따른다. (??)
마침내 RiRoI Align 은 rotation-invariant features 을 추출하기 위해 채택됐다
RoI-wise classification 와 bounding box regression 을 위해</p>
<blockquote>
<p>아는게 없어서 계속 같은 말 하는 것 처럼 느껴진다.</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>HRoIs</li>
<li>RoI Transformer</li>
<li>RPN</li>
</ul>
</li>
</ul>
<h3 id="1-rotation-equivariant-backbone" style="position:relative;"><a href="#1-rotation-equivariant-backbone" aria-label="1 rotation equivariant backbone permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(1) Rotation-equivariant Backbone</h3>
<p>현대 객체 탐색은 종종 semantic information 을 갖는 deep features 을 자동적으로 추출하기 위해 deep CNN 을 backbone 으로 채택한다.
예를 들어 ResNet with Feature Pyramid Network (FPN)
우리 또한 ResNet을 시작점으로 채택하고  rotation-equivariant backbone 을 구현했다.
Rotationequivariant ResNet 인 ReResNet 라고 부른다.
우리는 rotation-equivariant networks 로 backbone 의 모든 레이어를 재구현했다.
그 네트워크는 convolution, pooling, nomalization, non linearlities 를 포함하는 e2cnn 에 기반해서 이루어져있다.
computational budget 를 고려했을 때 ReResNet 와 ReFPN 은 오직 discrete group 에만 등변한다.
Fig 3 에서 보이는 것처럼 우리는 rotation-equivariant feature maps 을 만들기 위해 rotation-equivariant backbone 에 이미지를 줬다.
(??)
평범한 backbone 과 비교했을 때, rotation equivariant backbone 은 다음과 같은 이점이 있다.
더 높은 가중치의 공유.
rotation-equivariant feature maps 는 부가적인 방향 차원을 갖고있다.
다양한 방향으로부터의 feature 들은 종종 다른 rotation transformation 과 같은 필터를 공유한다.
풍부한 방향 정보.
고정된 방향을 갖는 인풋 이미지들에게 rotation-equivariant backbone 은 다양한 방향으로 부터의 feature 를 만들 수 있다.
이것은 정확한 방향 정보가 필요한 방향성을 갖는 객체 탐색에 중요하다.
작은 모델 사이즈.
baseline과 비교해서 우리는 backbone 을 설계할 때 두 가지 선택지가 있었다.
비슷한 computation 이냐 비슷한 parameters 냐 하는.
우리는 baseline 과 비슷한 computation 을 유지했다.
예를 들어 같은 output channel 를 지키는.
the rotation weight sharing 때문에 우리의 rotation-equivariant backbone 은 모델 사이즈를 대략 1/N 정도로 엄청나게 감소 시킬 수 있었다.</p>
<blockquote>
<p>Rotation-equivariant Backbone 어떻게 만들었는지 그리고 그 효과</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>semantic information</li>
<li>ResNet with Feature Pyramid Network (FPN)</li>
<li>e2cnn</li>
<li>convolution, pooling, nomalization, non linearlities</li>
<li>computational budget</li>
<li>discrete</li>
<li>output channel</li>
</ul>
</li>
</ul>
<h3 id="2--rotation-invariant-roi-align" style="position:relative;"><a href="#2--rotation-invariant-roi-align" aria-label="2  rotation invariant roi align permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(2)  Rotation-invariant RoI Align</h3>
<p>Sec. 3 에 소개된 것처럼 우리는 RRoI warping 으로 감싸진 rotation equivariant feature maps 로부터 rotation-invariant RoI features 를 얻을 수 있었다.
하지만 일반적인 RRoI wrapping 은 spartial dimension 에서 feature 들을 정렬시킬 수 없다.
그러므로 우리는 완전하게 rotation-invariant features 를 얻기 위해 RiRoI Align 을 제시한다.
Fig 3 에 보여지는 것 처럼 RiRoI Align 는 두 부분으로 나뉜다.
Spatial alignment.
spatial alignment 는 rotation-invariant
region features fR 를 만들기위해 feature maps f 로부터 감싼다.
Orientation alignment.
다양한 방향에서의 RRoIs 가 완전하게 rotation-invariant features 를 만든다는 것을 보장하기 위해 우리는 방향 차원에서 방향 정렬을 수행한다.
특히 output region feature 에서, 우리는 방향 정렬을 공식화했다.
(공식)
(???)</p>
<blockquote>
<p>공식을 이용한 유도</p>
</blockquote>
<p>RRoI Align+MaxPool 와의 비교.
RoI features 를 감싸고 방향 차원을 maxpooling 하는 rotationinvariant features 을 얻는 다른 접근 법이다.
orientation pooling operation 은 종종 classification 일을 하는 데에 사용된다.
feature map 의 각 location 에서 이것은 오직 가장 강한 응답을 하는 방향만 보존한다.
하지만 모든 방향으로부터의 응답은 객체 인식에 필수적이다.
우리의 RiRoI Align에서 모든 방향으로부터의 feature 는 보존되고 정렬된다.
우리는 RiRoI Align 의 이점을 보여주기 위해 실험을 진행했다. (Sec 5)</p>
<blockquote>
<p>다른 거는 가장 반응이 강한 방향만 사용하는데 우리는 모든 방향 사용한다.</p>
</blockquote>
<ul>
<li>몰라요
<ul>
<li>orientation alignment in orientation dimension</li>
<li>maxpooling</li>
<li>orientation pooling operation</li>
</ul>
</li>
</ul>
<hr>
<h2 id="experiments-and-analysis" style="position:relative;"><a href="#experiments-and-analysis" aria-label="experiments and analysis permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Experiments and Analysis</h2>
<h3 id="1-datasets" style="position:relative;"><a href="#1-datasets" aria-label="1 datasets permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(1) Datasets</h3>
<p>DOTA 는 항공 이미지에서 방향성을 갖는 객체 탐색을 위한 가장 큰 데이타셋이다.
DOTA-v1.0 과 DOTA-v1.5 두 버전이 있다.
DOTA-v1.0 은 2806 장의 항공 이미지
사이즈 800x800 ~ 4000x4000
instance 188,282 개
15개의 common category (plane, baseball diamond, bridge, ...)
DOTA-v1.5 는 새로운 카테고리와 함께 DOAI Challenge 2019 때문에 나왔다.
더 극도로 작은 instance - 402,089 개의 instance
DOTA-v1.5 가 더 challenging 하지만 트레이닝 동안에 더 안정돼다.
training 을 위해 training 과 validation set 둘 다를 썼다.
testing 을 위해 test set 를 썼다.
우리는 원래 이미지를 a stride of 824 를 가진 1024 x 1024 patch 로 잘랐다.
Random horizontal flipping 은 over fitting 을 피하기 위해 사용됐다.
또 다른 trick 은 이용되지 않았다.
다른 방법들과 공평한 비교를 위해 우리는 세 scale 에서 multi scale 데이터를 준비했다.
그리고 testing 과 trainging 을 위한 random rotation 을 준비했다
HRSC2016 는 challenging 배 데이터셋이다.
OBB annotations 가지고 있다.
1061 개의 항공 이미지.
사이즈는 300x300 ~ 1500x900
training 436 개, validation 181 개 그리고 test 444 개의 이미지가 있다.
training 을 위해 training 과 validation set 사용
testing 을 위해 testing set 사용
모든 이미지는 aspect ratio 변화 없이 800 x 512 로 리사이징 됐다.
Random horizontal flipping 이 트레이닝 동안 적용됐다.</p>
<ul>
<li>몰라요
<ul>
<li>multi scale 데이터</li>
<li>instance</li>
<li>validation</li>
<li>stride</li>
<li>Random horizontal flipping</li>
</ul>
</li>
</ul>
<h3 id="2-implementation-details" style="position:relative;"><a href="#2-implementation-details" aria-label="2 implementation details permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(2) Implementation Details</h3>
<p>ImageNet pretrain.
보통의 ResNet 에서, 우리는 pytorch 로 직접적으로 ImageNet pretrained 모델을 사용한다.
ReResNet 에서, 우리는 mmclassification 을 기반으로 구현했다.
우리는 초기 learning rate 0.1 로 ImageNet-1K 에서 ReResNet 을 훈련했다.
모든 모델은 100 epochs 로 훈련됐다.
learning rate 는 30, 60, 90 epoch 에서 10으로 나눠졌다.
batch size 는 256 으로 설정됐다.</p>
<p>Fine-tuning on detection.
우리는 기본 method 의 backbone 으로 FPN 과 함께 ResNet 를 선택했다.
ResNet 과 함께하는 ReResNet 는 우리가 제시한 ReDet 의 backbone 으로 선택됐다.
RPN 을 위해 우리는 피라미드 레벨의 각 location 마다 15 anchor 를 설정했다.
R-CNN 을 위해 우리는 트레이닝을 위한 1:3 positive to negative ratio 로 512 RoIs 를 샘플로 만들었다.
테스팅을 위해 우리는 NMS 전에 10000 RoIs 를 그리고 NMS 후에 2000 RoIs 를 선택했다.
우리는 mmdetection 과 같은 트레이닝 스케쥴을 선택했다.
SGD optimizer 는 초기 learning rate 0.01 로 선택됐다.
learning rate 는 매 decay 단계 마다 10으로 나뉘어졌다.
momentum 은 0.9 이다.
weight edcay 는 0.0001 이다.
우리는 모든 모델을 DOTA 에는 12 epochs 로 HRSC2016 는 36 epochs 로 훈련시켰다.
우리는 트레이닝을 위해 total batch size 8 로 4V100 GPUs 를 사용했다.
inference 를 위해 single V100 GPU 를 사용했다.</p>
<ul>
<li>몰라요
<ul>
<li>ImageNet pretrain</li>
<li>epoch</li>
<li>RPN</li>
<li>R-CNN</li>
<li>positive to negative ratio</li>
<li>mmdetection</li>
<li>decay step</li>
<li>inference</li>
</ul>
</li>
</ul>
<h3 id="3-ablation-studies" style="position:relative;"><a href="#3-ablation-studies" aria-label="3 ablation studies permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(3) Ablation Studies</h3>
<p>우리는 우리가 제시한 방법의 효율성을 평가하기 위해 DOTA-v1.5 test set 에서 일련의 ablation experiments.
우리가 일바넞ㄱ인 ResNet+FPN, backbone 으로 RRoI Align, 기본 method 로 RoI warping method 를 사용했단 것에 주목해라.
Rotation-equivariant backbone.
우리는 다른 세팅아래에 ReResNet50+ReFPN 로 rotation-equivariant backbone 의 효율성을 평가했다.
Tab 1 에서 보여지는 것처럼, ResNet50 과 비교했을 때 ResNet50 은 parameter 감소 때문에 더 낮은 classification accuracy 를 갖는다.
하지만 더 높은 detection mAP 를 얻는다.
우리는 cyclic group C8 아래에 있는 backbone 이 더 나은 accuracy-parameter-trade-off 를 얻는다는 거을 알아냈다.
게다가 우리는 Tab 2 에서처럼 ReResNet+ReFPN 를 우리의 방법을 확장시켰다.
Faster R-CNN OBB 와 ReResNet50+ReFPN 와 함께한 RetinaNet OBB 둘 다 이거의 counterpart 를 능가했다.</p>
<p>Effectiveness of RiRoI Align.
Tab 3 에서 보이는 것처럼 RRoI Align 와 비교했을 때, RiRoI Align 은 방향 정렬 메카니즘 때문에 상당한 개선을 보인다.
RRoI Align+MaxPool 이 mAP 에서 상당히 떨어졌기 때문에 orientation pooling 이 방향성을 갖는 객체 탐색에 바람지하기 못하단걸 알 수 있다.
l = 2 interpolation 으로 한 RiRoI Align 은 가장 높은 66.86 mAP 을 얻었고 RRoI Align 보다 0.87 mAP 더 개선됐다.
게다가 l = 4 interpolation 으로 한 RiRoI Align 이 오직 0.33 mAP 만을 얻었다는 것을 알아냈다.
그 이유는 아마 너무 많은 interpolations 가 equivariant property 와 안에 방향 사이에 relation 을 망쳤기 때문일 것이다.</p>
<p>Comparison with rotation augmentation.
다른 관점에서, 우리의 방법은 한 방향에서 학습하면 다른 방향에 적용할 수 있는 특별한 in-network rotation augmentation 으로 보여질 수 있다.
대조적으로 rotation augmentation 은 더 많은 방향을 갖는 샘플을 만들어서 network 를 강화시킨다.
그리고 종종 더 많은 시간을 필요로 한다.
Tab 4 에서 보이는 것처럼 비록 우리의 방법이 1x schedule 아래 rotation augmented 된 baseline 에 지나지 않을 지라도, 우리의 ReDet (비슷한 양의 parameter 를 보존하는) 는 오직 18% 추가 training time 으로 2.59mAP 개선을 보여줬다.
게다가 rotation augmentation 을 한 2xbaseline 은 ReDet 보다 0.68 더 높다.
하지만 이것은 두 배의 시간을 필요로 한다.</p>
<p>Performance on other datasets.
우리가 제시한 방법의 일반화를 증명하기 위해 우리는 ReDet on DOTA-v1.0 and HRSC2016 의 수행을 측정했다.
Tab5 에서 보이는 것처럼 baseline 과 비교해서 ReDet 은 양쪽에서 더 나은 performance 를 보인다.
게다가 ReDet 은 AP75 와 mAP 에서 상당히 개선됐다.</p>
<ul>
<li>몰라요
<ul>
<li>cyclic group</li>
<li>mAP</li>
<li>interpolations</li>
<li>inner relation between orientations</li>
<li>in-network rotation augmentation</li>
<li>localization capabilities.</li>
</ul>
</li>
</ul>
<h3 id="4-comparisons-with-the-state-of-the-art" style="position:relative;"><a href="#4-comparisons-with-the-state-of-the-art" aria-label="4 comparisons with the state of the art permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(4) Comparisons with the State-of-the-Art</h3>
<p>Results on DOTA-v1.0.
Tab6 에서 보이는 것처럼 우리는 우리의 ReDet 을 DOTA-v1.0 OBB Task 상에서 다른 좋은 방법들과 비교했다.
벨과 호루라기 없이 우리의 signle-scale model 은 76.25mAP 를 얻었다.
모든 single-scale model 을 능가하고 대부분의 multi-scale models 능가한다.
제한된 데이터 augmentation 으로 우리의 방법은 모든 데이터셋에서 80.10 mAP 를 얻었고 12/15 category 중 1, 2등을 했다.</p>
<p>Results on DOTA-v1.5.
DOTA-v1.0 과 비교했을 때 DOTA-v1.5 는 객체 탐색에 어려움을 증가시키는 많은 극도로 작은 instance 를 포함한다.
우리는 Tab 7 에서 DOTA-v1.5 에 대한 OBB, HBB 결과를 보고했다.
signle-scale data 에서 우리의 방법은 66.86 oBB mAP 오 67.77 HBB mAP 를 얻었다.
RetinaNet OBB, Faster R-CNN OBB, Mask R-CNN and HTC 를 능가한다.
특히 small instance 와 큰 scale variation 을 갖는 category 에서 우리의 방법은 더 잘 수행된다.
게다가 Fig 2 에서 보이는 것처럼 우리의 ReDet 은 더 나은 parameter vs accuracy trade off 를 얻었다.
이전 OWSR 에 의한 최고의 결과와 비교했을 때, 우리의 multi-scale model 은 최고의 성능을 달성했다.
대략 76.80 oBB mAP 와 78.08 HBB mAP.
ReDet 과 baseline 과의 질적 비교는 Fig 4 에 있다.</p>
<p>Result on HRSC2016.
HRSC2016 는 많은 얇고 긴 배 instance 를 포함한다.
우리는 Tab 8 에서 RedDet 과 다른 최신 방법과 비교했다.
우리의 방법은 최신 성능을 달성했다.
VOC2007, VOC2012 아래에서 각각 90.46 mAP 와 97.63 mAP</p>
<ul>
<li>몰라요
<ul>
<li>signle-scale model, multi-scale models</li>
<li>VOC2007, VOC2012</li>
</ul>
</li>
</ul>
<hr>
<h2 id="conclusions" style="position:relative;"><a href="#conclusions" aria-label="conclusions permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusions</h2>
<p>ReDet 두 부분으로 이루어져있다.</p>
<ol>
<li>rotation-equivariant backbone</li>
<li>RiRoI Align</li>
</ol>
<p>전자는 rotation-equivariant features 만든다
후자는 rotation-equivariant features 로 rotation-invariant features 얻는다.</p>
<p>DOTA 와 HRSC2016 에서 진행한 광범위한 실험은 이 방법이 효과적임을 보인다.</p></div><div class="comments-section"><h3>Comments</h3><div><form class="register-comment-form-comp"><div class="form"><label class="writer-label"><span class="label">Writer. </span><input/></label><label class="content-label"><span class="label">Content</span><textarea></textarea></label><div class="submit-button-container"><button type="submit" class="submit-button">Register</button></div><div class="caution">작성하신 댓글은 스스로 삭제하실 수 없습니다! 삭제를 원하시면 ajtwlswjddnv1102@gmail.com 로 연락주세요! 댓글 기능은 Github Issue API 를 이용해 만들었습니다! 댓글 등록이 안되면 연락주세요! 댓글이 바로 반영되지 않을 수도 있어요! API requests limitation 이 있어서 너무 많이 보시면 댓글이 안보입니다!</div></div></form></div></div></div><div class="home-button"><a class="hanjungwoo" href="/">Jungwoo Han</a></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/posts/8/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-6638744adb335e6f2a3c.js\"],\"component---src-page-template-articles-tsx\":[\"/component---src-page-template-articles-tsx-6cf2e8c1adc70bd97ff9.js\"],\"component---src-page-template-home-tsx\":[\"/component---src-page-template-home-tsx-208c925aa2a4c14b05dc.js\"],\"component---src-page-template-post-tsx\":[\"/component---src-page-template-post-tsx-8500d8b317cb7a551864.js\"],\"component---src-pages-404-tsx\":[\"/component---src-pages-404-tsx-9386a3134173d62fbda6.js\"],\"component---src-pages-squart-tsx\":[\"/component---src-pages-squart-tsx-c427e23b8745b9639ffa.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="16d5800077c0e3fe61df";</script><script src="/webpack-runtime-4ee86b2e16b8dc1fee21.js" async></script><script src="/framework-193fd97338660090d6b9.js" async></script><script src="/app-6638744adb335e6f2a3c.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>